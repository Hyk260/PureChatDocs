import{_ as l,o as e,c as t,am as o}from"./chunks/framework.CYljUBBp.js";const p=JSON.parse('{"title":"在 PureChat 中使用 Ollama","description":"","frontmatter":{},"headers":[],"relativePath":"guides/olama-usage.md","filePath":"guides/olama-usage.md","lastUpdated":1724925432000}'),i={name:"guides/olama-usage.md"};function s(r,a,h,m,d,n){return e(),t("div",null,a[0]||(a[0]=[o('<h1 id="在-purechat-中使用-ollama" tabindex="-1">在 PureChat 中使用 Ollama <a class="header-anchor" href="#在-purechat-中使用-ollama" aria-label="Permalink to &quot;在 PureChat 中使用 Ollama&quot;">​</a></h1><p>Ollama 是一款强大的本地运行大型语言模型（LLM）的框架，支持多种语言模型，包括 Llama 2, Mistral 等。现在，PureChat 已经支持与 Ollama 的集成，这意味着你可以在 PureChat 中轻松使用 Ollama 提供的语言模型来增强你的应用。</p><p>本文档将指导你如何在 PureChat 中使用 Ollama：</p><h2 id="在-macos-下使用-ollama" tabindex="-1">在 macOS 下使用 Ollama <a class="header-anchor" href="#在-macos-下使用-ollama" aria-label="Permalink to &quot;在 macOS 下使用 Ollama&quot;">​</a></h2><h3 id="本地安装-ollama" tabindex="-1">本地安装 Ollama <a class="header-anchor" href="#本地安装-ollama" aria-label="Permalink to &quot;本地安装 Ollama&quot;">​</a></h3><p><a href="https://ollama.com/download?utm_source=purechat&amp;utm_medium=docs&amp;utm_campaign=download-macos" target="_blank" rel="noreferrer">下载 Ollama for macOS</a> 并解压、安装。</p><h3 id="配置-ollama-允许跨域访问" tabindex="-1">配置 Ollama 允许跨域访问 <a class="header-anchor" href="#配置-ollama-允许跨域访问" aria-label="Permalink to &quot;配置 Ollama 允许跨域访问&quot;">​</a></h3><p>由于 Ollama 的默认参数配置，启动时设置了仅本地访问，所以跨域访问以及端口监听需要进行额外的环境变量设置 <code>OLLAMA_ORIGINS</code>。使用 <code>launchctl</code> 设置环境变量：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">launchctl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> setenv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> OLLAMA_ORIGINS</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;*&quot;</span></span></code></pre></div><p>完成设置后，需要重启 Ollama 应用程序。</p><h3 id="在-purechat-中与本地大模型对话" tabindex="-1">在 PureChat 中与本地大模型对话 <a class="header-anchor" href="#在-purechat-中与本地大模型对话" aria-label="Permalink to &quot;在 PureChat 中与本地大模型对话&quot;">​</a></h3><p>接下来，你就可以使用 PureChat 与本地 LLM 对话了。</p><h2 id="在-windows-下使用-ollama" tabindex="-1">在 windows 下使用 Ollama <a class="header-anchor" href="#在-windows-下使用-ollama" aria-label="Permalink to &quot;在 windows 下使用 Ollama&quot;">​</a></h2><h3 id="本地安装-ollama-1" tabindex="-1">本地安装 Ollama <a class="header-anchor" href="#本地安装-ollama-1" aria-label="Permalink to &quot;本地安装 Ollama&quot;">​</a></h3><p><a href="https://ollama.com/download?utm_source=purechat&amp;utm_medium=docs&amp;utm_campaign=download-windows" target="_blank" rel="noreferrer">下载 Ollama for Windows</a> 并安装。</p><h3 id="配置-ollama-允许跨域访问-1" tabindex="-1">配置 Ollama 允许跨域访问 <a class="header-anchor" href="#配置-ollama-允许跨域访问-1" aria-label="Permalink to &quot;配置 Ollama 允许跨域访问&quot;">​</a></h3><p>由于 Ollama 的默认参数配置，启动时设置了仅本地访问，所以跨域访问以及端口监听需要进行额外的环境变量设置 <code>OLLAMA_ORIGINS</code>。</p><p>在 Windows 上，Ollama 继承了您的用户和系统环境变量。</p><ol><li>首先通过 Windows 任务栏点击 Ollama 退出程序。</li><li>从控制面板编辑系统环境变量。</li><li>为您的用户账户编辑或新建 Ollama 的环境变量 <code>OLLAMA_ORIGINS</code>，值设为 <code>*</code> 。</li><li>点击<code>OK/应用</code>保存后重启系统。</li><li>重新运行<code>Ollama</code>。</li></ol><h3 id="在-purechat-中与本地大模型对话-1" tabindex="-1">在 PureChat 中与本地大模型对话 <a class="header-anchor" href="#在-purechat-中与本地大模型对话-1" aria-label="Permalink to &quot;在 PureChat 中与本地大模型对话&quot;">​</a></h3><p>接下来，你就可以使用 PureChat 与本地 LLM 对话了。</p><h2 id="安装-ollama-模型" tabindex="-1">安装 Ollama 模型 <a class="header-anchor" href="#安装-ollama-模型" aria-label="Permalink to &quot;安装 Ollama 模型&quot;">​</a></h2><p>Ollama 支持多种模型，你可以在 <a href="https://ollama.com/library" target="_blank" rel="noreferrer">Ollama Library</a> 中查看可用的模型列表，并根据需求选择合适的模型。</p><h3 id="用-ollama-拉取模型到本地" tabindex="-1">用 Ollama 拉取模型到本地 <a class="header-anchor" href="#用-ollama-拉取模型到本地" aria-label="Permalink to &quot;用 Ollama 拉取模型到本地&quot;">​</a></h3><p>当然，你也可以通过在终端执行以下命令安装模型，以 llama3 为例：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pull</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3</span></span></code></pre></div><div class="tip custom-block"><p class="custom-block-title">TIP</p><p>你可以前往 <a href="/pure-docs/guides/ollama.html">与 Ollama 集成</a> 了解如何部署 PureChat，以满足与 Ollama 的集成需求。</p></div>',27)]))}const u=l(i,[["render",s]]);export{p as __pageData,u as default};
